# HackUMass_ThirdEye

An iOS app for empowering visually impaired to help finding their belongings and overcome day-to-day struggles through Mobile camera as "Third Eye" using Object Detection Algorithm and speech recognition.

![Screen Shot 2019-10-19 at 11 24 03 PM](https://user-images.githubusercontent.com/17843556/67154305-f7201780-f2c7-11e9-9833-0eb273222a0a.png)
 
# Introduction
Our project takes input from User to find a specific item using speech-to-text and detects the requested object in the room in "REAL-TIME" and using beep sensing and vibration in the mobile phone it will guide the user to the item. <br/>
@authors: <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i>Prashuk Ajmera<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Hitesh Verma<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Parag Bhingarkar<br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Genglin Liu<br/></i>
         
# <a name="prerequisites"></a>Prerequisites
## Platform requirements
The program was developed on XCODE 11.2 with Swift 5.1 programming language and it is available for iOS 11.0 and above
The following frameworks are used to build the application
- Swift 5.1
- AVKit
- AVFoundation
- Speech
- CoreML
- TFLite

# Demo 
- [Youtube link](https://youtu.be/b543EcMP2SA)
- [Project Source](https://dashboard.hackumass.com/projects/54)
